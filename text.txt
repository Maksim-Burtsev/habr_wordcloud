Введение 

 

Вопрос обработки информации с каждым годом становится всё более и более актуальным, потому что вследствие цифровизации и человеческого развития количество информации в Интернете беспрерывно растёт.  

Совсем недавно анализу подвергались лишь заранее собранные данные, и на их основе люди делали какие-то выводы и гипотезы. Серьёзными ограничением на первых порах были скромные вычислительные мощности (по сравнению с тем, что мы имеем сейчас) и высокий порог входа в индустрию - нужно было не только обладать знаниями в области языка, но и иметь навыки владения используемыми технологиями, чтобы писать эффективные программы.  

Сегодня же мы анализируем данные в реальном времени. Практически не задумываемся об оптимизации, потому что имеем в своём распоряжении эффективные высокоуровневые инструменты. И самое главное - стремимся достичь наиболее эффективных результатов в анализе. 

 В отношении русского языка эта тема особенно актуальна, потому что все ведущие разработки в этой области ведутся на английском языке. Технологии получают поддержку других языков в случае успешности или наличием энтузиастов, которые эту поддержку и реализуют. 

Так же существуют локальные разработки в рамках того или иного языка, но они редко попадают в свет, потому что разрабатываются и используются внутри корпораций, крупных компаний или банков.  

В работе используются открытые технологии, которые имеют поддержку русского языка. С их помощью будет проведен частотный анализ и анализ на основе веса слов. 

Цель работы: провести семантический анализ текста новостных сообщений  и сделать вывод относительно используемых методов. 

Задачи: 

Провести обзор выбранных методов  

Собрать и подготовить данные для анализа 

Визуализировать результаты 

Сравнить полученные данные и выявить их недостатки/преимущества. 

 

Сематнтический анализ 

Семантический анализ является подзадачей (или же частным случаем) NLP, которому прежде всего нужно дать определение. 

NLP (Natural Language Processing - “обработка естественного языка”) — это направление в машинном обучении, посвященное распознаванию, генерации и обработке устной и письменной человеческой речи. Находится на стыке дисциплин искусственного интеллекта и лингвистики. 

NLP включает в себя ряд основных подходов: 

Предобработка текста (переводит текст в удобный для машины вид) 

Стемминг (приводит слово к его основной форме) 

Лемматизация (альтернатива стемминга, которая приводит слово к словарной форме - Именительный падеж, единственное число, мужской род, инфинитив) 

Векторизация (отображение текста в векторном пространстве) 

Дедубликация (удаление дубликатов из текста, который представлен в векторном пространстве) 

Семантический анализ (смысловой анализ текста) 

Распознование именованных сущеностей и извлечение отношений (именованные сущности - объекты из текста, которые могут быть отнесены к одной из заранее заявленных категорий, например - организации, личности, адреса). 

И др. 

Подходы пересекаются между собой и в рамках задачи их всегда используется несколько. 

Семантический анализ — это последовательность действий алгоритма автоматического понимания текстов, которая выделяет смысловые отношения в тексте.  

В общем случае семантическое представление является графом, семантической сетью, отражающим бинарные отношения между двумя узлами — смысловыми единицами текста. Глубина семантического анализа может быть разной, а в реальных системах чаще всего строится только лишь синтаксико-семантическое представление текста или отдельных предложений. 

 

Перед анализом документ чаще всего проходит предварительную очистку, в ходе которой удаляются все служебные символы, цифры представляются в виде текста, а слова приводятся к своей начально форме. Так же возможно удаление стоп-слов — это слова, которые не вносят никакой дополнительной информации в текст и только добавляют “шум” в данные. 

 

В ходе анализа текст проходит через несколько этапов обработки: токенизация для идентификации словоформ, морфологический, синтаксический анализ.  

Токенизация — это процесс разбиения текстового документа на отдельные слова или предложения. Разбиение происходит по пунктуационному разделителю: в случае предложения точке, слова - пробелу.  

Последним этапом идет вторичный семантический анализ (первичный анализ в основном происходит параллельно морфологическому), в ходе которого устанавливаются взаимосвязи между сущностями, происходит извлечение мнений и анализ тональности текста. Основной целью анализа тональности является не только определение настроений, но также уровень объективности высказывания. 

Семантический анализ активно применяется для создания онлайн-консультантов или чат-ботов, поисковых и рекомендательных системах и анализе тональности текста.  

В данной работе будут рассмотрены частотный анализ и анализ на основе веса слов.  

Частотный анализ представляет собой подсчёте количества вхождений слова в текст и формировании выводов на основе самых часто встречающихся слов. Чаще всего этот метод применяется в криптоанализе и работает с буквами.  

Вес слова — это его значимость в рамках данного текста. Для его вычисления подсчитывается число вхождений каждого слова не только в конкретном документе, но и в похожих текстах. Такой подход называется «Частота термина — обратная частота документа» (term frequency — inverse document frequency, TF-IDF). 

Так, повторяющиеся в тексте слова «вознаграждаются», но также «штрафуются», если часто употребляются в коллекции схожих текстов, которую включают в алгоритм. Так системе NLP проще найти уникальные слова и оценить их важность в тексте. 

Похожие текста, которые используются при сравнении, представляют собой “дата-сет”, на котором модель была обучена. 

 

2. Сбор и анализ данных  

1.1. Инструментарий 

Когда речь заходит об анализе данных, машинном обучении, Big-Data, нейронных сетях и в целом науке, то сразу приходит на ум Python, который является лидером в этой области. Простой синтаксис, большое комьюнити, высокая скорость прототипирования и большое количество библиотек и пакетов — всё это выводит его на первый план.  

Стоит отметить, что сам по себе язык интерпретируемый, и как следствие, серьёзно проигрывает по скорости компилируемым языкам вроде C++ или Rust. А в рамках большого количества данных скорость и оптимизация стоят чуть ли не на первом месте.  

Но почему же тогда большинство людей использует именно его? А потом что большинство популярных библиотек, особенно тех, которые требуют скорости, написаны на компилируемых языках, Python же выступает в роли “обёртки”, которая позволяет использовать быстрый низкоуровневый код, написав буквально пару строчек.  

Для сбора данных будут использоваться библиотеки: selenium и bs4. 

Для анализа данных: nltk (c помощью него будет выполняться токенизация и частотный анализ) и gensim (с пощью инструментов этой библиотеки будет выполняться частотный анализ). 

Все модели, которые будут использованы, изначально обучены на “лучшем” дата-сете. Лучшим является тот дата-сет, который впоследствии даёт наилучшие результаты.  

Так как большинство моделей являются регресионными, вопрос качества данных для обучения стоит чуть ли не на первом месте. В свободном доступе есть огромное количество датасетов, в том числе от компании Google. 

Для визуализации: библиотека WordCloud, которая создаёт “облако слов” на основе полученных данных, которые находятся в нужном ей виде. 

1.2 Парсинг данных 

В качестве платформы, с которой мы будем брать новостные сообщения был выбран Habr. Он интересен нам не только из-за своей популярности, но и из-за разнообразия тем публикаций, которые пишутся на самые разные тематики.  

Так как веб-страница формируется при помощи JavaScript, “спарсить” её не так просто - вместо содержания статьи мы получаем код, который нужно “прогрузить”. Для этого мы воспользуемся библиотекой selenium, которая эмулирует работу браузера, тем самым прогружает весь фронтенд приложения.  

Затем при помощи библиотеки bs4 мы “достаём” из страницы блок с текстом, который и необходим анализа. 

Так как мы имеем дело с полноценной html-страницей, то прежде всего нам нужно очистить её от тегов и лишних пробелов. Для этого воспользуемся силой регулярных выражений.  

res = re.sub(r'<[^>]*\>', '', article).strip() 

res = re.sub(r' +', ' ', res) 

 

Полученный “чистый текст” записывается в файл и дальше мы переходим к подготовке текста к анализу. 

 

2.2. Частотный анализ 

Подготовка текста к частотному анализу и анализу на основе веса слов немного отличается, поэтому мы рассмотрим их отдельно в рамках каждого метода.  

Для частотного анализа текст проходит следующие этапы обработки: 

Приведение слов к нижнему регистру 

Удаление пунктуационных символов (в том числе перенос строки \n и таб \t)  и пробелов 

Удаление английских букв. 

Чаще всего английские буквы и термины формируют только “шум”, который в дальнейшем даёт некорректные результаты.  

Токенизация 

Выполняется она при помощи функции из библиотеки nltk и выполняет разбиение текста на структурные единицы (чаще всего это слова или предложения) 

Удаление стоп-слов 

Стоп-слова, как и английские символы, создают сильный “шум”, особенно при частотном анализе. Любое злоупотребление союзами или вводными словами автоматически приводит к не самым лучшим результатам.  

Стоп слова достаются из библиотки nltk и отдельной бибиотеки stop_words.  

Берутся слова из двух библиотек, потому что наборы между собой отличаются. Наша же цель - убрать как можно больше стоп-слов, поэтому мы стремимся взять в наш словарь их как можно больше.   

Подсчет количества вхождений каждого слова 

Подсчитать количество вхождений каждого слова можно в одну строку, используя инструментарий Python, но мы же воcпользуемся классом FreqDist из библиотеки nltk, потому что помимо подсчёта он ещё раз токенизирует текст и приводит все слова к нижнему регистру. 

Стоит отметить, что результат подсчёта предстаёт перед нами не в целых числах, а в десятичных дробях, которые лежат в диапазоне от 0 до 1. Переход от целых чисел к дробям происходит относительно самого большого количества вхождений слова в текст.   

Казалось бы, мы делали токенизацию самостоятельно, зачем делать её ещё раз? После повторения этого процесса результат улучшился - некоторые слова не пришли и инфинитив с первого раза и оказались в нём только со второго. Скорее всего это произошло из-за особенностей работы библиотеки с русским языком. 

В результате мы получаем словарь, где ключами являются слова, а значениями количество раз, которое они встретились в тексте.  

(тут запятые между токенами будут и их частота) 

[оленей олени поэтому стадо оленя примерно  тундре снег вообще посёлок делают зимой посёлка километров посёлке например едят либо фап хатанге учатся балок видно тепло метров павел петрович ехать речь  

летом дня ветра стада вместе посёлки некоторые берут привозят сразу хатангу посёлков ночь тяжело учим оленеводы нарты рыбы поехали балка какойто ресурсов еды двигаться случае снегом обратно мясо источник кочевать искать дома важно дом новая вроде стоит горит оленевода далее хотите сети выглядит чай 

волка ходить песцы берег одинаковые бывают сани быков оставляем олень дела собственно прививки база  

живут середине снегоход собрались буране автомобильный аккумулятор] 

Давайте внимательно рассмотрим результаты частотного анализа.  

В первых десятках слов у нас есть большое количество практически идентичных слов: “оленей”, “олени”, “оленя”, “посёлке”, “посёлки”, “посёлок” и т.д. Попали они сюда по простой причине того, что автор уделил особое внимание этим словам. Результат частотного анализа всегда напрямую зависит от того, какой “язык” у автора и на чём он хочет сделать акцент. Чаще всего это только мешает понять основной смысл публикации, особенно если текст наполнен большим количеством вводных слов и наречий.  

Во-вторых, в тексте встречается большое количество стоп-слов вроде “вообще”, “поэтому”, “либо”, “некоторые” и т.д. Попали они сюда из-за недостаточной наполненности русского слова “шумных” слов. Встречаться мы с ними мы будем на протяжении всей работы. 

 И наконец обратим внимание на смысловую нагрузку, которую мы можем извлечь из результата. Речь в статье идёт об оленеводах и оленях, жизни в тундре летом и зимой, конкретном посёлке, некоем Павле Петровиче, а также мясе и рыбе. Поверхностное представление составить можно.  

 

А сейчас рассмотрим анализ на основе веса слов.  

2.3 Анализ на основе веса слова 

Сперва происходит очистка и токенизация текста. Они выполняются точно так же, как и в частотном анализе. 

Далее нам необходимо создать корпус. Корпус - это подобранная и обработанная по определённым правилам совокупность текстов, используемых в качестве базы для исследования языка. В случае с используемой нами библиотекой gensim корпус содержит идентификатор (id) слова и его частоту в каждом документе. 

Пример того, как выглядит корпус: 

[ 
[(0, 1), (1, 1), (2, 1), (3, 1)], 
[(2, 1), (3, 1), (4, 2)], [(0, 2), (3, 3), (5, 2), (6, 1), (7, 2), (8, 1)] 
] 

Теперь нам необходимо передать этот корпус модели TfidfModel, которая вернёт нам взвешенную матрицу TF-IDF, содержащую только положительные значения. Но прежде, чем сделать это давайте остановимся нам том, что это за матрица.  

TF-IDF (от англ. TF — term frequency, IDF — inverse document frequency) — статистическая мера, используемая для оценки важности слова в контексте документа, являющегося частью коллекции документов или корпуса. Вес некоторого слова пропорционален частоте употребления этого слова в документе и обратно пропорционален частоте употребления слова во всех документах коллекции. 

Мера TF-IDF часто используется в задачах анализа текстов и информационного поиска, например, как один из критериев релевантности документа поисковому запросу, при расчёте меры близости документов при кластеризации. 

Частота слова вычисляется по следующей формуле: 

 

TF-IDF — Википедия (wikipedia.org) (вот остюда будут взяты описание и формулы) 

TfidfModel возвращает нам id токена (слова) и его вес. Для удобства выполним переход от идентификаторов к исходным словам нашего корпуса, попутно удалив все английские слова, которые случайно попали в текст.  

'фап': 1.0, 'хотя': 1.0, 'новая': 0.766, 'да': 0.76, 'едешь': 0.755, 'охраняем': 0.738, 'вот': 0.714, 'космос': 0.707, 'русскии': 0.707, 'бураном': 0.707, 'догонял': 0.707, 'деревня': 0.707, 'эх': 0.707, 'все': 0.7, 'домашние': 0.683, 'начинается': 0.681, 'одинаковые': 0.671, 'прививки': 0.662, 'ну': 0.65, 'он': 0.648, 'больных': 0.647, 'лечат': 0.647, 'называется': 0.646, 'цивилизация': 0.646, 'поселок': 0.643, 'бы': 0.635, 'иду': 0.629, 'твердыи': 0.628, 'минут': 0.615, 'бывают': 0.613, 'видела': 0.602, 'издалека': 0.602, 'некоторые': 0.601, 'едут': 0.596, 'мастерицеи': 0.596, 'приходится': 0.594, 'соответственно': 0.594, 'ведро': 0.594, 'улицу': 0.594, 'воздух': 0.583, 'встаешь': 0.577,  

'печку': 0.577, 'топишь': 0.577, 'воо': 0.577, 'геолога': 0.577, 'смеется': 0.577, 'ложишься': 0.572, 'вкус': 0.571, 'важных': 0.571, 'проверяешь': 0.571, 'озерах': 0.57, 'реках': 0.57, 'рыбу': 0.57,  

'дню': 0.57, 'ко': 0.57, 'подряд': 0.569, 'грех': 0.562, 'считать 

 

Во-первых, мы видим наличие слов с ошибками: русскии, твердыи, мастерицей и т.д. В исходном тексте подобных ошибок не было - они появились в результате работы библиотеки. Как мы увидим в дальнейших анализах, буква Й будет заменяться на И довольно часто, однако без каких-либо последствий для анализа. Поэтому будем считать это особенностью.  

Во-вторых, в тексте встречаются слова, которые по отдельности не имеют смысла: хотя, да, вот, ну, бы и т.д.  

В-третьих, давайте дадим оценку соответствия содержанию статьи и результатам анализа. Публикация называется “Арктичекая база” без современных технологий: как живут оленеводы. В ней рассказывается о том, кто такие оленеводы и где и как они живут.  

Среди первого десятка слов оленей как мы видим не наблюдается, однако есть посёлок, реки, озёра, печка и даже рыба, что уже неплохо. По этим словам уже можно частично раскрыть содержание статьи. Так же сразу бросается в глаза, что “случайных” слов среди самых весомых оказалось значительно меньше по сравнению с частотным анализом.  

Следующим этапом нашей работы будет визуализация полученных результатов и их сравнение при анализе разных статей.  

 

3.Визуализация данных и сравнение результатов 

Для визуализации данных мы будем использовать облако слов, на котором размер слова прямо пропорционален его важности в тексте.  

Библиотека WordCloud позволяет сделать это в одну строку, нужно лишь передать все необходимые параметры. В них входит: 

 текст, который содержит в себе уникальные токены в порядке от самого важного к наименее (чтобы привести наши результаты к такому виду достаточно “склеить” все слова в порядке убывания их веса/частоты) 

Размер облака (расширение картинки) 

Размер отступов между словами 

Список стоп-слов (которые не попадут в облако) 

“Цветовая карта” (в Python есть широкораспространённый пакет matplotlib, который используется для виализации различных графиков, диаграмм и т.д. Библиотека WordCloud использует аналогичные цветовые карты у себя) 

Это основные параметры, на деле библиотека предоставляет куда более гибкую настройку.  

Визуализируем наши результаты частотного и TD-IDF анализа.  

 

(веса) 

 

(частотный) 

Теперь после того как результаты анализа стало воспринимать намного легче, возьмём несколько публикаций на разные темы и сделаем выводы на основе результатов.  

3.2 Статьи и результаты 

Начнём с публикации под названием “Современное пиратство глазами моряка”, в которой идёт речь о борьбе с пиратами, которые встречаются в Суэцком канале. 
Результаты частотного анализа.  

Внимательно рассмотрим все слова на первом плане.  

“Пиратов, обороны, пиратам, пираты, судно, мостик, военные” - единственное, что можно констатировать наверняка, особый акцент автора на пиратах и обороне, но не более того. Из этого результата анализа становится понятно, что итог напрямую зависит от того, что написал автор, а не какой смысл он в этой заложил. 

Так же на облаке встречается большее количество прилагательных и наречий, которые не представляют ценности по отдельности и имеют вес лишь в словосочетаниях. Например слова: самое, сильно, военные, вторую, полностью, поэтому и т.д. 

Наречия же вроде слова “поэтому” попадают в результат по причине отсутствия их в списке стоп-слов, от которых мы очищали текст несколько раз. Очевидно, что словари стоп-слов для русского языка нужно расширять.  

 

Результаты TDF-ID анализа. 

Данная статья представляет из себя большую ценность как текст, потому что в ней содержится много эпитетов, метафор, гипербол и существительных, которые не находятся в инфинитиве.  

Последнее особенно важно и интересно, потому что в данной публикации встречается 25 слов с корнем “пират”. Но почему же мы не наблюдаем этого слова на облаке где-то в центре?  

Дело в том, что модель даёт больший вес подлежащим, которые находятся в именительном падеже или инфинитиве. Именно поэтому мы наблюдаем на первом плане такие слова как “никого”, “рук”, “тренировки”, “связь”, “деньги”, “маневры” и т.д., и никаких пиратов здесь нет.  
 
Подобная проблема при анализе связана в первую очередь с особенностями нашего языка.  В английском языке, который является “родным” для данной библиотеки, мы чаще всего имеем дело со словами в исходной форме, а изменение лица или числа слова вносит в него минимальные изменения. В русском языке все намного сложнее и падежи играют большую роль.  

Как сегодня эту проблему можно обойти? 

Изменение данных, которые использует модель при анализе. Хорошо подготовленный “дата-сет” может не только учитывать падежи подлежащих, но и поможет избавиться от наречий и прилагательных, которые случайно получают большой вес 

Лемматизация. Приведение слова к нормальной форме сразу же снимет все вопросы, которые касаются форм слова. Однако изначально tdf-id анализ рассчитан на работу со всеми формами слов, поэтому связка с лемматизацией возможна лишь как промежуточный этап в рамках другого анализа.  

С глаголами ситуация обстоит значительно лучше. Самыми крупными (т.е. значимыми) являются “влетают”, “прорвана”, “догнали”, “называется”, “освободят”, “бдительность”, “согласились” и т.д.  - все они соответствуют содержанию статьи. Но без контекста и наличия каких-либо существительных понять по ним о чём речь в тексте.   

В заключении рассмотрим техническую статью, которая отлично подходит для анализа - в ней всё написано максимально прямо, и она не пестрит второстепенными членами предложения.  

Публикация называется “Как мы отказались от JPEG, JSON, TCP и ускорили ВКонтакте в два раза”. 

 

Результаты частотного анализа. 

Первое, что бросается в глаза - слово “МС”, которое может обозначать как минимум четыре вещи: “мастер спорта”, “миллисекунда”, аббревиатура или же опечатку, которая случайным образом попала в текст.  

Так как анализ частотный и слово “мс” встретилось больше всего раз в тексте, то поверить в то, что автор ошибся столько раз сложно. Рядом есть слова “сети”, “сетевой и “скорость”, поэтому скорее всего речь идёт о миллисекундах.  

Однако это без проблем может быть какой-либо аббревиатурой, которую мы просто-напросто не знаем. Так как текста анализируются в нижнем регистре (практически при любом более-менее серьёзном анализе), то аббревиатуры физически не видно в результатах и постоянно приходится разбираться самостоятельно.  

Обойти эту проблему можно двумя способами: удалять аббревиатуры при очищении текста, либо же помечать их и учитывать при анализе. Как мы видим, ни того ни другого используемые библиотеки не реализуют. 

Второе, что нельзя не заметить - стоп-слова, которые остались после очистки: “примерно”, “поэтому”, “например”, “вообще” и т.д.  

В-третьих, этот случай достаточно хорош, чтобы по результатам анализа можно хотя бы частично узнать суть статьи. У нас есть очевидный акцент на скорости, “сжатие контента”, “сервер, сети, маршрутизатор, мбитс, пропускная”, “пакеты, алгоритмы” - уже по всеми этим словам можно составить общее представление о том, какая информация содержится в тексте, пусть и на самом поверхностном уровне.  

Давайте посмотрим для сравнения на результаты TDF-ID. 

 

Здесь результаты еще интереснее, потому что практически отсутствуют слова, которые создают “шум”. Оба алгоритма анализировали идентичный текст и использовали один и тот же список стоп-слов, который нужно удалить (а он как мы выяснили далеко не полный). И о том, как эффективно TDF-ID алгоритм может не пропускать что-то лишнее в результат, можно увидеть по этому результату.  

C первого взгляда очевидно, что в публикации пойдёт речь об использовании альтернативных технологиях, сравнение их с технологиями от которых воздержались, пингование/исследование их эффективности, а также создание своих собственных “фич”.  

Зная название статьи и посмотрев беглым взглядом на статью, любой человек, находящийся в технологической сфере, сможет уловить основную мысль статьи.  

Стоп-слова на облаке по прежнему есть, но их значительно меньше по сравнению с предыдущими статьями, что напрямую связано с тем, что их практически не было в тексте.  

 

4. Выводы 

В процессе... 

 
 