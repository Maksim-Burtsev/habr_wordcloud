Введение 

 

Вопрос обработки информации с каждым годом становится всё более и более актуальным, потому что вследствие цифровизации и человеческого развития количество информации в Интернете беспрерывно растёт.  

Совсем недавно анализу подвергались лишь заранее собранные данные, и на их основе люди делали какие-то выводы и гипотезы. Серьёзными ограничением на первых порах были скромные вычислительные мощности (по сравнению с тем, что мы имеем сейчас) и высокий порог входа в индустрию - нужно было не только обладать знаниями в области языка, но и иметь навыки владения используемыми технологиями, чтобы писать эффективные программы.  

Сегодня же мы анализируем данные в реальном времени. Практически не задумываемся об оптимизации, потому что имеем в своём распоряжении эффективные высокоуровневые инструменты. И самое главное - стремимся достичь наиболее эффективных результатов в анализе. 

 В отношении русского языка эта тема особенно актуальна, потому что все ведущие разработки в этой области ведутся на английском языке. Технологии получают поддержку других языков в случае успешности или наличием энтузиастов, которые эту поддержку и реализуют. 

Так же существуют локальные разработки в рамках того или иного языка, но они редко попадают в свет, потому что разрабатываются и используются внутри корпораций, крупных компаний или банков.  

В работе используются открытые технологии, которые имеют поддержку русского языка. С их помощью будет проведен частотный анализ и анализ на основе веса слов. 

Цель работы: провести семантический анализ текста новостных сообщений  и сделать вывод относительно используемых методов. 

Задачи: 

Провести обзор выбранных методов  

Собрать и подготовить данные для анализа 

Визуализировать результаты 

Сравнить полученные данные и выявить их недостатки/преимущества. 

 

Сематнтический анализ 

Семантический анализ является подзадачей (или же частным случаем) NLP, которому прежде всего нужно дать определение. 

NLP (Natural Language Processing - “обработка естественного языка”) — это направление в машинном обучении, посвященное распознаванию, генерации и обработке устной и письменной человеческой речи. Находится на стыке дисциплин искусственного интеллекта и лингвистики. 

NLP включает в себя ряд основных подходов: 

Предобработка текста (переводит текст в удобный для машины вид) 

Стемминг (приводит слово к его основной форме) 

Лемматизация (альтернатива стемминга, которая приводит слово к словарной форме - Именительный падеж, единственное число, мужской род, инфинитив) 

Векторизация (отображение текста в векторном пространстве) 

Дедубликация (удаление дубликатов из текста, который представлен в векторном пространстве) 

Семантический анализ (смысловой анализ текста) 

Распознование именованных сущеностей и извлечение отношений (именованные сущности - объекты из текста, которые могут быть отнесены к одной из заранее заявленных категорий, например - организации, личности, адреса). 

И др. 

Подходы пересекаются между собой и в рамках задачи их всегда используется несколько. 

Семантический анализ — это последовательность действий алгоритма автоматического понимания текстов, которая выделяет смысловые отношения в тексте.  

В общем случае семантическое представление является графом, семантической сетью, отражающим бинарные отношения между двумя узлами — смысловыми единицами текста. Глубина семантического анализа может быть разной, а в реальных системах чаще всего строится только лишь синтаксико-семантическое представление текста или отдельных предложений. 

 

Перед анализом документ чаще всего проходит предварительную очистку, в ходе которой удаляются все служебные символы, цифры представляются в виде текста, а слова приводятся к своей начально форме. Так же возможно удаление стоп-слов — это слова, которые не вносят никакой дополнительной информации в текст и только добавляют “шум” в данные. 

 

В ходе анализа текст проходит через несколько этапов обработки: токенизация для идентификации словоформ, морфологический, синтаксический анализ.  

Токенизация — это процесс разбиения текстового документа на отдельные слова или предложения. Разбиение происходит по пунктуационному разделителю: в случае предложения точке, слова - пробелу.  

Последним этапом идет вторичный семантический анализ (первичный анализ в основном происходит параллельно морфологическому), в ходе которого устанавливаются взаимосвязи между сущностями, происходит извлечение мнений и анализ тональности текста. Основной целью анализа тональности является не только определение настроений, но также уровень объективности высказывания. 

Семантический анализ активно применяется для создания онлайн-консультантов или чат-ботов, поисковых и рекомендательных системах и анализе тональности текста.  

В данной работе будут рассмотрены частотный анализ и анализ на основе веса слов.  

Частотный анализ представляет собой подсчёте количества вхождений слова в текст и формировании выводов на основе самых часто встречающихся слов. Чаще всего этот метод применяется в криптоанализе и работает с буквами.  

Вес слова — это его значимость в рамках данного текста. Для его вычисления подсчитывается число вхождений каждого слова не только в конкретном документе, но и в похожих текстах. Такой подход называется «Частота термина — обратная частота документа» (term frequency — inverse document frequency, TF-IDF). 

Так, повторяющиеся в тексте слова «вознаграждаются», но также «штрафуются», если часто употребляются в коллекции схожих текстов, которую включают в алгоритм. Так системе NLP проще найти уникальные слова и оценить их важность в тексте. 

Похожие текста, которые используются при сравнении, представляют собой “дата-сет”, на котором модель была обучена. 

 

2. Сбор и анализ данных  

1.1. Инструментарий 

Когда речь заходит об анализе данных, машинном обучении, Big-Data, нейронных сетях и в целом науке, то сразу приходит на ум Python, который является лидером в этой области. Простой синтаксис, большое комьюнити, высокая скорость прототипирования и большое количество библиотек и пакетов — всё это выводит его на первый план.  

Стоит отметить, что сам по себе язык интерпретируемый, и как следствие, серьёзно проигрывает по скорости компилируемым языкам вроде C++ или Rust. А в рамках большого количества данных скорость и оптимизация стоят чуть ли не на первом месте.  

Но почему же тогда большинство людей использует именно его? А потом что большинство популярных библиотек, особенно тех, которые требуют скорости, написаны на компилируемых языках, Python же выступает в роли “обёртки”, которая позволяет использовать быстрый низкоуровневый код, написав буквально пару строчек.  

Для сбора данных будут использоваться библиотеки: selenium и bs4. 

Для анализа данных: nltk (c помощью него будет выполняться токенизация и частотный анализ) и gensim (с пощью инструментов этой библиотеки будет выполняться частотный анализ). 

Все модели, которые будут использованы, изначально обучены на “лучшем” дата-сете. Лучшим является тот дата-сет, который впоследствии даёт наилучшие результаты.  

Так как большинство моделей являются регресионными, вопрос качества данных для обучения стоит чуть ли не на первом месте. В свободном доступе есть огромное количество датасетов, в том числе от компании Google. 

Для визуализации: библиотека WordCloud, которая создаёт “облако слов” на основе полученных данных, которые находятся в нужном ей виде. 

1.2 Парсинг данных 

В качестве платформы, с которой мы будем брать новостные сообщения был выбран Habr. Он интересен нам не только из-за своей популярности, но и из-за разнообразия тем публикаций, которые пишутся на самые разные тематики.  

Так как веб-страница формируется при помощи JavaScript, “спарсить” её не так просто - вместо содержания статьи мы получаем код, который нужно “прогрузить”. Для этого мы воспользуемся библиотекой selenium, которая эмулирует работу браузера, тем самым прогружает весь фронтенд приложения.  

Затем при помощи библиотеки bs4 мы “достаём” из страницы блок с текстом, который и необходим анализа. 

Так как мы имеем дело с полноценной html-страницей, то прежде всего нам нужно очистить её от тегов и лишних пробелов. Для этого воспользуемся силой регулярных выражений.  

res = re.sub(r'<[^>]*\>', '', article).strip() 

res = re.sub(r' +', ' ', res) 

 

Полученный “чистый текст” записывается в файл и дальше мы переходим к подготовке текста к анализу. 

 

2.2. Частотный анализ 

Подготовка текста к частотному анализу и анализу на основе веса слов немного отличается, поэтому мы рассмотрим их отдельно в рамках каждого метода.  

Для частотного анализа текст проходит следующие этапы обработки: 

Приведение слов к нижнему регистру 

Удаление пунктуационных символов (в том числе перенос строки \n и таб \t)  и пробелов 

Удаление английских букв. 

Чаще всего английские буквы и термины формируют только “шум”, который в дальнейшем даёт некорректные результаты.  

Токенизация 

Выполняется она при помощи функции из библиотеки nltk и выполняет следующие преобразования: 

(тут фотография из Jupiter) 

Удаление стоп-слов 

Стоп-слова, как и английские символы, создают сильный “шум”, особенно при частотном анализе. Любое злоупотребление союзами или вводными словами автоматически приводит к не самым лучшим результатам.  

Стоп слова достаются из библиотки nltk и отдельной бибиотеки stop_words.  

Берутся слова из двух библиотек, потому что наборы между собой отличаются. Наша же цель - убрать как можно больше стоп-слов, поэтому мы стремимся взять в наш словарь их как можно больше.   

Подсчет количества вхождений каждого слова 

Подсчитать количество вхождений каждого слова можно в одну строку, используя инструментарий Python, но мы же воcпользуемся классом FreqDist из библиотеки nltk, потому что помимо подсчёта он ещё раз токенизирует текст и приводит все слова к нижнему регистру. 

Стоит отметить, что результат подсчёта предстаёт перед нами не в целых числах, а в десятичных дробях, которые лежат в диапазоне от 0 до 1. Переход от целых чисел к дробям происходит относительно самого большого количества вхождений слова в текст.   

Казалось бы, мы делали токенизацию самостоятельно, зачем делать её ещё раз? После повторения этого процесса результат улучшился - некоторые слова не пришли и инфинитив с первого раза и оказались в нём только со второго. Скорее всего это произошло из-за особенностей работы библиотеки с русским языком. 

В результате мы получаем словарь, где ключами являются слова, а значениями количество раз, которое они встретились в тексте.  

 

['фап', 1.0], ['хотя', 1.0], ['новая', 0.766], ['да', 0.76], ['едешь', 0.755], ['охраняем', 0.738], ['вот', 0.714], ['космос', 0.707], ['русскии', 0.707], ['бураном', 0.707], ['догонял', 0.707], ['деревня', 0.707], ['эх', 0.707], ['все', 0.7], ['домашние', 0.683], ['начинается', 0.681], ['одинаковые', 0.671], ['прививки', 0.662], ['ну', 0.65], ['он', 0.648], ['больных', 0.647], ['лечат', 0.647], ['называется', 0.646], ['цивилизация', 0.646], ['поселок', 0.643], ['бы', 0.635], ['иду', 0.629], ['твердыи', 0.628], ['минут', 0.615], ['бывают', 0.613], ['видела', 0.602], ['издалека', 0.602], ['некоторые', 0.601], ['едут', 0.596], ['мастерицеи', 0.596], ['приходится', 0.594], ['соответственно', 0.594], ['ведро', 0.594], ['улицу', 0.594], ['воздух', 0.583], ['встаешь', 0.577], ['печку', 0.577], ['топишь', 0.577], ['воо', 0.577], ['геолога', 0.577], ['смеется', 0.577], ['ложишься', 0.572], ['вкус', 0.571], ['важных', 0.571], ['проверяешь', 0.571], ['озерах', 0.57], ['реках', 0.57], ['рыбу', 0.57], ['дню', 0.57], ['ко', 0.57], ['подряд', 0.569], ['грех', 0.562] 

 

Давайте внимательно рассмотрим результаты частотного анализа.  

Во-первых, мы видим наличие слов с ошибками: фап, русскии, твердыи, мастерицеи и т.д. Коэффиценты этих слов явно говорят о том, что слова эти встречались в тексте достаточно частно и вряд ли автор написал их столько раз с ошибками. И так как мы никак не трогали слова во время очистки и предоставили эту возможность библиотеке можно сделать следующий вывод - на данный момент она не совсем корректно работает с русским языком и токенизация не всегда проходит успешно.  

Во-вторых, в тексте встречаются слова, которые по отдельности не имеют смысла: хотя, да, вот, ну, бы и т.д. Они могут пригодится разве что при анализе стиля написания автора в контексте того, какими союзами и вводными словами автор чаще всего пользуется.  

В-третьих, давайте дадим оценку соответствия содержанию статьи и результатам анализа. Публикация называется “Арктичекая база” без современных технологий: как живут оленеводы. В ней рассказывается о том, кто такие оленеводы и где и как они живут. Среди первого десятка слов оленей как мы видим не наблюдается, однако есть посёлок, реки, озёра, печка и даже рыба, что уже неплохо. Но относительно полноты и подробностей публикации мы получили лишь самые поверхностные сведения.  

Возможно, что нам попалась не совсем удачная статья и результаты могут быть лучше.  

К результатам частотного анализа мы ещё вернёмся в пункте “Визуализация и сравнение”, где будет рассматриваться несколько статей и результаты будут в виде облаков слов. А сейчас рассмотрим анализ на основе веса слов.  

 

 

 

 
 